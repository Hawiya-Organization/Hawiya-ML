{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede1ad9f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cf1ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import sentencepiece as sp\n",
    "\n",
    "import tkseem as tk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           poem_text\n",
       "0  خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...\n",
       "1  فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...\n",
       "2  وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...\n",
       "3  لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا\n",
       "4  وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/apcd_clean.csv')\n",
    "df['poem_text'] = df['البيت'].astype(str)\n",
    "df.drop('البيت', inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c18ffe",
   "metadata": {},
   "source": [
    "## Morphological Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8254d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MorphologicalTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "morph_tokenizer = tk.MorphologicalTokenizer(vocab_size=500_000)\n",
    "\n",
    "morph_tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ab903e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['توكلت', 'في', 'رزق', '##ي', 'على', 'الله', 'خالق', '##ي']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'توكلت في رزقي على الله خالقي'\n",
    "\n",
    "morph_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text_morph(text):\n",
    "    return morph_tokenizer.tokenize(text)\n",
    "\n",
    "df['morph_t'] = df.poem_text.apply(tokenize_text_morph)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98942021",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'أق'\n",
    "\n",
    "morph_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea62c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_model = gensim.models.Word2Vec(\n",
    "    df['morph_t'],\n",
    "    window=5,\n",
    "    min_count=4,\n",
    "    workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69cd5cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('حريق', 0.7407509088516235),\n",
       " ('لازورد', 0.7376511693000793),\n",
       " ('سبج', 0.7300616502761841),\n",
       " ('جلنار', 0.7153370380401611),\n",
       " ('زجاج', 0.7149295806884766),\n",
       " ('جام', 0.7102290391921997),\n",
       " ('لجين', 0.7094608545303345),\n",
       " ('ياسمين', 0.7039445638656616),\n",
       " ('شرر', 0.7033098936080933),\n",
       " ('ذوب', 0.7032054662704468)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_model.wv.most_similar(positive='ثلج')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7c1292",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_model.save('models/morph_word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696fada",
   "metadata": {},
   "source": [
    "## Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06708a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = tk.WordTokenizer()\n",
    "\n",
    "df['poem_text'].to_csv('data.txt', sep=' ', index=False, header=False)\n",
    "word_tokenizer.train('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b3cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem_text</th>\n",
       "      <th>morph_t</th>\n",
       "      <th>word_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...</td>\n",
       "      <td>[خليلي, لا, &lt;UNK&gt;, أن, تزودا, و, ##أن, تجمعا, ...</td>\n",
       "      <td>[خليلي, لا, &lt;UNK&gt;, أن, &lt;UNK&gt;, وأن, &lt;UNK&gt;, شملي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...</td>\n",
       "      <td>[&lt;UNK&gt;, لبث, يوما, &lt;UNK&gt;, مغنم, &lt;UNK&gt;, سرعت, #...</td>\n",
       "      <td>[فما, &lt;UNK&gt;, يوما, &lt;UNK&gt;, &lt;UNK&gt;, ولا, &lt;UNK&gt;, ي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...</td>\n",
       "      <td>[وإن, تنظران, ##ي, ال, ##يوم, أقض, لبانة, &lt;UNK...</td>\n",
       "      <td>[وإن, &lt;UNK&gt;, اليوم, أقض, لبانة, &lt;UNK&gt;, منا, عل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا</td>\n",
       "      <td>[لع, ##مر, ##ك, ما, نفس, &lt;UNK&gt;, رشيدة, تؤامر, ...</td>\n",
       "      <td>[لعمرك, ما, نفس, بجد, &lt;UNK&gt;, &lt;UNK&gt;, سرا, &lt;UNK&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ...</td>\n",
       "      <td>[وإن, ظهرت, منه, قوارص, جمة, و, ##أفرع, في, لو...</td>\n",
       "      <td>[وإن, ظهرت, منه, &lt;UNK&gt;, جمة, &lt;UNK&gt;, في, لومي, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           poem_text  \\\n",
       "0  خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...   \n",
       "1  فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...   \n",
       "2  وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...   \n",
       "3  لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا   \n",
       "4  وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ...   \n",
       "\n",
       "                                             morph_t  \\\n",
       "0  [خليلي, لا, <UNK>, أن, تزودا, و, ##أن, تجمعا, ...   \n",
       "1  [<UNK>, لبث, يوما, <UNK>, مغنم, <UNK>, سرعت, #...   \n",
       "2  [وإن, تنظران, ##ي, ال, ##يوم, أقض, لبانة, <UNK...   \n",
       "3  [لع, ##مر, ##ك, ما, نفس, <UNK>, رشيدة, تؤامر, ...   \n",
       "4  [وإن, ظهرت, منه, قوارص, جمة, و, ##أفرع, في, لو...   \n",
       "\n",
       "                                              word_t  \n",
       "0  [خليلي, لا, <UNK>, أن, <UNK>, وأن, <UNK>, شملي...  \n",
       "1  [فما, <UNK>, يوما, <UNK>, <UNK>, ولا, <UNK>, ي...  \n",
       "2  [وإن, <UNK>, اليوم, أقض, لبانة, <UNK>, منا, عل...  \n",
       "3  [لعمرك, ما, نفس, بجد, <UNK>, <UNK>, سرا, <UNK>...  \n",
       "4  [وإن, ظهرت, منه, <UNK>, جمة, <UNK>, في, لومي, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text_word(text):\n",
    "    return word_tokenizer.tokenize(text)\n",
    "\n",
    "df['word_t'] = df.poem_text.apply(tokenize_text_word)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6fa9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.Word2Vec(\n",
    "    df['word_t'],\n",
    "    window=5,\n",
    "    min_count=4,\n",
    "    workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c33e45b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('بهاء', 0.7381551265716553),\n",
       " ('جلال', 0.7211630344390869),\n",
       " ('كمال', 0.7165018320083618),\n",
       " ('جمالا', 0.6697500944137573),\n",
       " ('جماله', 0.6606019735336304),\n",
       " ('محيا', 0.6495076417922974),\n",
       " ('سناء', 0.6364024877548218),\n",
       " ('بهجة', 0.6349396109580994),\n",
       " ('ملاحة', 0.6105407476425171),\n",
       " ('ضياء', 0.6032314896583557)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_model.wv.most_similar(positive='جمال')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752f8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.save('models/word_word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd5a91",
   "metadata": {},
   "source": [
    "## Sentence Piece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e5e02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: models/sp_poem_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 50000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1831727), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1831727 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=86935952\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9925% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=37\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999925\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1831727 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=39308960\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 856697 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1831727\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 751143\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 751143 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=459341 obj=11.6444 num_tokens=1233101 num_tokens/piece=2.6845\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=384649 obj=10.3095 num_tokens=1236095 num_tokens/piece=3.21357\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=288467 obj=10.2043 num_tokens=1286423 num_tokens/piece=4.45952\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=288071 obj=10.177 num_tokens=1286751 num_tokens/piece=4.46678\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=216052 obj=10.2215 num_tokens=1368556 num_tokens/piece=6.33438\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=216049 obj=10.203 num_tokens=1368860 num_tokens/piece=6.33588\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=162036 obj=10.2732 num_tokens=1444181 num_tokens/piece=8.91272\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=162036 obj=10.2507 num_tokens=1445818 num_tokens/piece=8.92282\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=121527 obj=10.3439 num_tokens=1515822 num_tokens/piece=12.4731\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=121527 obj=10.3152 num_tokens=1515979 num_tokens/piece=12.4744\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=91145 obj=10.4315 num_tokens=1581078 num_tokens/piece=17.3468\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=91145 obj=10.3965 num_tokens=1581411 num_tokens/piece=17.3505\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=68358 obj=10.5367 num_tokens=1645912 num_tokens/piece=24.0778\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=68358 obj=10.495 num_tokens=1647090 num_tokens/piece=24.0951\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=55000 obj=10.6138 num_tokens=1694417 num_tokens/piece=30.8076\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55000 obj=10.5791 num_tokens=1696043 num_tokens/piece=30.8371\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: models/sp_poem_model.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: models/sp_poem_model.vocab\n"
     ]
    }
   ],
   "source": [
    "sp.SentencePieceTrainer.train(\n",
    "    sentence_iterator=iter(df['poem_text'].values),\n",
    "    model_prefix='models/sp_poem_model',\n",
    "    vocab_size=50_000,\n",
    "    pad_id=0,\n",
    "    unk_id=1,\n",
    "    bos_id=2,\n",
    "    eos_id=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bc7cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "spp = sp.SentencePieceProcessor(model_file='models/sp_poem_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5a8d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poem_text</th>\n",
       "      <th>morph_t</th>\n",
       "      <th>word_t</th>\n",
       "      <th>sp_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...</td>\n",
       "      <td>[خليلي, لا, &lt;UNK&gt;, أن, تزودا, و, ##أن, تجمعا, ...</td>\n",
       "      <td>[خليلي, لا, &lt;UNK&gt;, أن, &lt;UNK&gt;, وأن, &lt;UNK&gt;, شملي...</td>\n",
       "      <td>[▁خليلي, ▁لا, ▁تستع, جلا, ▁أن, ▁تزود, ا, ▁وأن,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...</td>\n",
       "      <td>[&lt;UNK&gt;, لبث, يوما, &lt;UNK&gt;, مغنم, &lt;UNK&gt;, سرعت, #...</td>\n",
       "      <td>[فما, &lt;UNK&gt;, يوما, &lt;UNK&gt;, &lt;UNK&gt;, ولا, &lt;UNK&gt;, ي...</td>\n",
       "      <td>[▁فما, ▁لبث, ▁يوما, ▁بسابق, ▁مغنم, ▁ولا, ▁سرع,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...</td>\n",
       "      <td>[وإن, تنظران, ##ي, ال, ##يوم, أقض, لبانة, &lt;UNK...</td>\n",
       "      <td>[وإن, &lt;UNK&gt;, اليوم, أقض, لبانة, &lt;UNK&gt;, منا, عل...</td>\n",
       "      <td>[▁وإن, ▁تنظر, اني, ▁اليوم, ▁أقض, ▁لبان, ة, ▁وت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا</td>\n",
       "      <td>[لع, ##مر, ##ك, ما, نفس, &lt;UNK&gt;, رشيدة, تؤامر, ...</td>\n",
       "      <td>[لعمرك, ما, نفس, بجد, &lt;UNK&gt;, &lt;UNK&gt;, سرا, &lt;UNK&gt;...</td>\n",
       "      <td>[▁لعمرك, ▁ما, ▁نفس, ▁بجد, ▁رشيد, ة, ▁تؤام, ر, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ...</td>\n",
       "      <td>[وإن, ظهرت, منه, قوارص, جمة, و, ##أفرع, في, لو...</td>\n",
       "      <td>[وإن, ظهرت, منه, &lt;UNK&gt;, جمة, &lt;UNK&gt;, في, لومي, ...</td>\n",
       "      <td>[▁وإن, ▁ظهرت, ▁منه, ▁قوارص, ▁جمة, ▁وأف, رع, ▁ف...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           poem_text  \\\n",
       "0  خليلي لا تستعجلا أن تزودا    وأن تجمعا شملي وت...   \n",
       "1  فما لبث يوما بسابق مغنم    ولا سرعتي يوما بساب...   \n",
       "2  وإن تنظراني اليوم أقض لبانة    وتستوجبا منا عل...   \n",
       "3  لعمرك ما نفس بجد رشيدة    تؤامرني سرا لأصرم مرثدا   \n",
       "4  وإن ظهرت منه قوارص جمة    وأفرع في لومي مرارا ...   \n",
       "\n",
       "                                             morph_t  \\\n",
       "0  [خليلي, لا, <UNK>, أن, تزودا, و, ##أن, تجمعا, ...   \n",
       "1  [<UNK>, لبث, يوما, <UNK>, مغنم, <UNK>, سرعت, #...   \n",
       "2  [وإن, تنظران, ##ي, ال, ##يوم, أقض, لبانة, <UNK...   \n",
       "3  [لع, ##مر, ##ك, ما, نفس, <UNK>, رشيدة, تؤامر, ...   \n",
       "4  [وإن, ظهرت, منه, قوارص, جمة, و, ##أفرع, في, لو...   \n",
       "\n",
       "                                              word_t  \\\n",
       "0  [خليلي, لا, <UNK>, أن, <UNK>, وأن, <UNK>, شملي...   \n",
       "1  [فما, <UNK>, يوما, <UNK>, <UNK>, ولا, <UNK>, ي...   \n",
       "2  [وإن, <UNK>, اليوم, أقض, لبانة, <UNK>, منا, عل...   \n",
       "3  [لعمرك, ما, نفس, بجد, <UNK>, <UNK>, سرا, <UNK>...   \n",
       "4  [وإن, ظهرت, منه, <UNK>, جمة, <UNK>, في, لومي, ...   \n",
       "\n",
       "                                                sp_t  \n",
       "0  [▁خليلي, ▁لا, ▁تستع, جلا, ▁أن, ▁تزود, ا, ▁وأن,...  \n",
       "1  [▁فما, ▁لبث, ▁يوما, ▁بسابق, ▁مغنم, ▁ولا, ▁سرع,...  \n",
       "2  [▁وإن, ▁تنظر, اني, ▁اليوم, ▁أقض, ▁لبان, ة, ▁وت...  \n",
       "3  [▁لعمرك, ▁ما, ▁نفس, ▁بجد, ▁رشيد, ة, ▁تؤام, ر, ...  \n",
       "4  [▁وإن, ▁ظهرت, ▁منه, ▁قوارص, ▁جمة, ▁وأف, رع, ▁ف...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_text_sp(text):\n",
    "    return spp.encode(text, out_type=str)\n",
    "\n",
    "df['sp_t'] = df.poem_text.apply(tokenize_text_sp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "faa347d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = gensim.models.Word2Vec(\n",
    "    df['sp_t'],\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c3b92fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('لأم', 0.8726752996444702),\n",
       " ('حصن', 0.8706651926040649),\n",
       " ('رجل', 0.8677271008491516),\n",
       " ('نقع', 0.8672091960906982),\n",
       " ('رماح', 0.8636544346809387),\n",
       " ('حجار', 0.8592782020568848),\n",
       " ('ظفار', 0.8591319918632507),\n",
       " ('جادل', 0.8589949607849121),\n",
       " ('خطار', 0.8574706315994263),\n",
       " ('درع', 0.8571063280105591)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_model.wv.most_similar(positive='أسد')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba1d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model.save('models/sp_word2vec_model.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534940c",
   "metadata": {},
   "source": [
    "## Save Updated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4525b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_poems_tokenized.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
