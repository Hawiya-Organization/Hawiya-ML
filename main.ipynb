{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"./all_poems.csv\")\n",
    "\n",
    "\n",
    "data=df['poem_text']\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"asafaya/bert-base-arabic\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=data.tolist()\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_indices=[random.randint(0,58021) for i in range(100)]\n",
    "\n",
    "\n",
    "data_list=data.iloc[random_indices].to_list()\n",
    "test_emeb=embedder.embed_documents(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_emeb))\n",
    "print(len(data[0]))\n",
    "print(len(test_emeb[0]))\n",
    "\n",
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a code to insert into a postgress db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Define your database credentials\n",
    "db_name = \"hawiya\"\n",
    "user = \"postgres\"\n",
    "password = \"password\"\n",
    "host = \"localhost\"\n",
    "port = \"5432\"\n",
    "\n",
    "# Establish a connection to the database\n",
    "conn = psycopg2.connect(database=db_name, user=user, password=password, host=host, port=port)\n",
    "cur = conn.cursor()\n",
    "#cur.execute(\"CREATE TABLE embeddings (id bigserial PRIMARY KEY, word TEXT, embedding vector(\" + str(768)+ \"));\")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over the word embeddings\n",
    "for  i,emebeding in enumerate(test_emeb):\n",
    "    # Convert the embedding to a string representation\n",
    "    \n",
    "    my_doc = {\"id\":i, \"text\" :data[i],\"embedding\":emebeding}\n",
    "    \n",
    "    # Insert the embedding into the database\n",
    "    cur.execute(\"\"\"INSERT INTO embeddings (id,word,embedding) VALUES (%(id)s, %(text)s, %(embedding)s)\"\"\", my_doc)\n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A smaple code that do similarity search using cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# Assuming your query is a string stored in the variable 'query'\n",
    "query=\"المغركة\"\n",
    "\n",
    "# Calculate the embedding for the query\n",
    "query_embedding = await embedder.aembed_documents([query])\n",
    "\n",
    "\n",
    "\n",
    "# Calculate cosine similarities between the query and all documents\n",
    "similarities = cosine_distances(query_embedding, test_emeb)\n",
    "print(similarities)\n",
    "\n",
    "# Get the index of the most similar document\n",
    "most_similar_index = similarities.argmax()\n",
    "\n",
    "# Print the most similar document\n",
    "print(data_list[most_similar_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use another emebderrer and re run the cells above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"elgeish/gpt2-medium-arabic-poetry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here I used a word emebedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "معتقد 0.6983626484870911\n",
      "ديانه 0.6978577971458435\n",
      "مذهب 0.6649177074432373\n",
      "ودين 0.6610172986984253\n",
      "الدين 0.6588460206985474\n",
      "دين_الاسلام 0.6555569767951965\n",
      "بدين 0.6483359336853027\n",
      "ملحد 0.6435856819152832\n",
      "ديانته 0.6386861801147461\n",
      "الاديان 0.6349233388900757\n",
      "الابزمو 0.0\n",
      "الدائم_الجريان 0.0\n",
      "وقطيعه 0.0\n",
      "مرحله_اخيره 0.0\n",
      "جندو 0.0\n",
      "السعره 0.0\n",
      "تل_العبر 0.0\n",
      "القملق 0.0\n",
      "واشوريه 0.0\n",
      "بالسرعه_القصوي 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.4409056 ,  3.9559472 ,  0.21397716, -0.521136  ,  0.36912638,\n",
       "       -3.4426553 ,  0.28245908,  0.6631324 ,  1.621887  ,  0.9472866 ,\n",
       "        0.31828615,  0.8806815 ,  2.157055  , -2.8736787 ,  2.787323  ,\n",
       "       -1.1993876 , -0.93114215, -2.9399326 , -2.1674523 , -0.296613  ,\n",
       "        2.231288  , -4.4799953 ,  2.0459082 , -2.9747512 , -0.15462679,\n",
       "        1.4269712 ,  1.4808508 ,  2.2602508 , -4.9776835 ,  4.579959  ,\n",
       "        1.8137403 ,  1.7823577 , -0.4286507 , -0.1906432 , -0.28240505,\n",
       "       -1.5433767 , -0.8022959 , -1.2146631 ,  0.75317276, -1.761817  ,\n",
       "        1.392355  ,  3.3090816 ,  1.9117038 , -3.9833324 ,  0.3098168 ,\n",
       "       -0.75069016, -3.1657631 , -1.3959129 ,  1.3551639 ,  0.9554129 ,\n",
       "        3.3537982 ,  4.023201  ,  0.82530016, -3.415946  , -2.3655834 ,\n",
       "        0.04621622,  2.1432161 ,  1.1844045 , -2.884357  , -0.7491137 ,\n",
       "        0.73062736, -0.17919174,  0.9194095 , -3.5709348 ,  0.6004428 ,\n",
       "        2.945328  ,  0.70685315,  0.5640709 ,  2.7145057 , -3.2366192 ,\n",
       "       -3.5843751 , -4.0790653 , -2.3288789 , -1.1753854 , -0.07293849,\n",
       "       -0.8155897 , -1.4986074 , -3.4060435 ,  4.751893  , -0.4105389 ,\n",
       "       -0.68970484, -2.9172544 ,  2.524077  ,  2.0129597 , -2.9764895 ,\n",
       "       -0.23386265,  2.7555923 ,  0.15809964, -3.090121  , -1.8963058 ,\n",
       "        1.2922721 , -3.902969  ,  1.681866  ,  1.9988799 , -0.8441784 ,\n",
       "        2.5401194 , -0.16820519,  1.5674069 ,  2.1014528 , -0.8863121 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from utilities import * \n",
    "\n",
    "model = gensim.models.Word2Vec.load('./models/full_grams_cbow_100_wiki.mdl')\n",
    "\n",
    "\n",
    "token = clean_str(u'دين').replace(\" \", \"_\")\n",
    "\n",
    "\n",
    "if token in model.wv:\n",
    "    most_similar = model.wv.most_similar( token, topn=10 )\n",
    "    for term, score in most_similar:\n",
    "        term = clean_str(term).replace(\" \", \"_\")\n",
    "        if term != token:\n",
    "            print(term, score)\n",
    "\n",
    "\n",
    "\n",
    "pos_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in [] if t.strip() != \"\"]\n",
    "neg_tokens=[ clean_str(t.strip()).replace(\" \", \"_\") for t in [''] if t.strip() != \"\"]\n",
    "\n",
    "vec = calc_vec(pos_tokens=pos_tokens, neg_tokens=neg_tokens, n_model=model, dim=model.vector_size)\n",
    "\n",
    "most_sims = model.wv.similar_by_vector(vec, topn=10)\n",
    "for term, score in most_sims:\n",
    "    if term not in pos_tokens+neg_tokens:\n",
    "        print(term, score)\n",
    "\n",
    "\n",
    "word_vector = model.wv[ token ]\n",
    "\n",
    "word_vector\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
